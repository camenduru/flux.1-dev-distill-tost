{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content\n",
    "!pip install torchsde einops diffusers transformers accelerate peft timm kornia aiohttp\n",
    "!apt install -qqy\n",
    "\n",
    "!git clone https://github.com/comfyanonymous/ComfyUI /content/ComfyUI\n",
    "!git clone https://github.com/ltdrdata/ComfyUI-Manager /content/ComfyUI/custom_nodes/ComfyUI-Manager\n",
    "!git clone https://github.com/asagi4/ComfyUI-Adaptive-Guidance /content/ComfyUI/custom_nodes/ComfyUI-Adaptive-Guidance\n",
    "!git clone https://github.com/Jonseed/ComfyUI-Detail-Daemon /content/ComfyUI/custom_nodes/ComfyUI-Detail-Daemon\n",
    "!git clone -b dev https://github.com/camenduru/ComfyUI_SLK_joy_caption_two /content/ComfyUI/custom_nodes/ComfyUI_SLK_joy_caption_two\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/consolidated_s6700.safetensors -d /content/ComfyUI/models/unet -o consolidated_s6700.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_g.safetensors -d /content/ComfyUI/models/clip -o clip_g.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp16.safetensors -d /content/ComfyUI/models/clip -o t5xxl_fp16.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/Long-ViT-L-14-BEST-GmP-smooth-ft.safetensors -d /content/ComfyUI/models/clip -o Long-ViT-L-14-BEST-GmP-smooth-ft.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/ComfyUI/models/vae -o ae.sft\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/detailed_v2_flux_ntc.safetensors -d /content/ComfyUI/models/loras -o detailed_v2_flux_ntc.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/openflux1-v0.1.0-fast-lora.safetensors -d /content/ComfyUI/models/loras -o openflux1-v0.1.0-fast-lora.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/xlabs_flux_realism_lora_comfui.safetensors -d /content/ComfyUI/models/loras -o xlabs_flux_realism_lora_comfui.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/FLUX.1-Turbo-Alpha.safetensors -d /content/ComfyUI/models/loras -o FLUX.1-Turbo-Alpha.safetensors\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/raw/main/config.json -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/raw/main/generation_config.json -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o generation_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/resolve/main/model-00001-of-00004.safetensors -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o model-00001-of-00004.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/resolve/main/model-00002-of-00004.safetensors -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o model-00002-of-00004.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/resolve/main/model-00003-of-00004.safetensors -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o model-00003-of-00004.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/resolve/main/model-00004-of-00004.safetensors -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o model-00004-of-00004.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/raw/main/model.safetensors.index.json -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o model.safetensors.index.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/raw/main/special_tokens_map.json -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o special_tokens_map.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/raw/main/tokenizer.json -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o tokenizer.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/Orenguteng/Llama-3.1-8B-Lexi-Uncensored-V2/raw/main/tokenizer_config.json -d /content/ComfyUI/models/LLM/Orenguteng--Llama-3.1-8B-Lexi-Uncensored-V2 -o tokenizer_config.json\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/google/siglip-so400m-patch14-384/raw/main/config.json -d /content/ComfyUI/models/clip/siglip-so400m-patch14-384 -o config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/google/siglip-so400m-patch14-384/resolve/main/model.safetensors -d /content/ComfyUI/models/clip/siglip-so400m-patch14-384 -o model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/google/siglip-so400m-patch14-384/raw/main/preprocessor_config.json -d /content/ComfyUI/models/clip/siglip-so400m-patch14-384 -o preprocessor_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/google/siglip-so400m-patch14-384/raw/main/special_tokens_map.json -d /content/ComfyUI/models/clip/siglip-so400m-patch14-384 -o special_tokens_map.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/google/siglip-so400m-patch14-384/resolve/main/spiece.model -d /content/ComfyUI/models/clip/siglip-so400m-patch14-384 -o spiece.model\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/google/siglip-so400m-patch14-384/raw/main/tokenizer.json -d /content/ComfyUI/models/clip/siglip-so400m-patch14-384 -o tokenizer.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/google/siglip-so400m-patch14-384/raw/main/tokenizer_config.json -d /content/ComfyUI/models/clip/siglip-so400m-patch14-384 -o tokenizer_config.json\n",
    "\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/joy-caption-alpha-two/resolve/main/image_adapter.pt -d /content/ComfyUI/models/Joy_caption_two -o image_adapter.pt\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/joy-caption-alpha-two/raw/main/config.yaml -d /content/ComfyUI/models/Joy_caption_two -o config.yaml\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/joy-caption-alpha-two/resolve/main/clip_model.pt -d /content/ComfyUI/models/Joy_caption_two -o clip_model.pt\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/joy-caption-alpha-two/raw/main/text_model/tokenizer_config.json -d /content/ComfyUI/models/Joy_caption_two/text_model -o tokenizer_config.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/joy-caption-alpha-two/raw/main/text_model/tokenizer.json -d /content/ComfyUI/models/Joy_caption_two/text_model -o tokenizer.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/joy-caption-alpha-two/raw/main/text_model/special_tokens_map.json -d /content/ComfyUI/models/Joy_caption_two/text_model -o special_tokens_map.json\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/joy-caption-alpha-two/resolve/main/text_model/adapter_model.safetensors -d /content/ComfyUI/models/Joy_caption_two/text_model -o adapter_model.safetensors\n",
    "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/joy-caption-alpha-two/raw/main/text_model/adapter_config.json -d /content/ComfyUI/models/Joy_caption_two/text_model -o adapter_config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /content/ComfyUI\n",
    "\n",
    "import os, shutil, json, requests, random, time\n",
    "from urllib.parse import urlsplit\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "from nodes import NODE_CLASS_MAPPINGS, load_custom_node\n",
    "from comfy_extras import nodes_flux, nodes_model_advanced, nodes_custom_sampler, nodes_sd3\n",
    "\n",
    "load_custom_node(\"/content/ComfyUI/custom_nodes/ComfyUI-Adaptive-Guidance\")\n",
    "load_custom_node(\"/content/ComfyUI/custom_nodes/ComfyUI-Detail-Daemon\")\n",
    "load_custom_node(\"/content/ComfyUI/custom_nodes/ComfyUI_SLK_joy_caption_two\")\n",
    "\n",
    "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]()\n",
    "TripleCLIPLoader = nodes_sd3.NODE_CLASS_MAPPINGS[\"TripleCLIPLoader\"]()\n",
    "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]()\n",
    "CLIPVisionLoader = NODE_CLASS_MAPPINGS[\"CLIPVisionLoader\"]()\n",
    "LoadImage = NODE_CLASS_MAPPINGS[\"LoadImage\"]()\n",
    "StyleModelLoader =  NODE_CLASS_MAPPINGS[\"StyleModelLoader\"]()\n",
    "LoraLoader = NODE_CLASS_MAPPINGS[\"LoraLoader\"]()\n",
    "Joy_caption_two_load = NODE_CLASS_MAPPINGS[\"Joy_caption_two_load\"]()\n",
    "Joy_caption_two = NODE_CLASS_MAPPINGS[\"Joy_caption_two\"]()\n",
    "\n",
    "ModelSamplingFlux = nodes_model_advanced.NODE_CLASS_MAPPINGS[\"ModelSamplingFlux\"]()\n",
    "CLIPTextEncode = NODE_CLASS_MAPPINGS[\"CLIPTextEncode\"]()\n",
    "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]()\n",
    "AdaptiveGuidance = NODE_CLASS_MAPPINGS[\"AdaptiveGuidance\"]()\n",
    "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]()\n",
    "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]()\n",
    "DetailDaemonSamplerNode = NODE_CLASS_MAPPINGS[\"DetailDaemonSamplerNode\"]()\n",
    "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
    "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]()\n",
    "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    unet = UNETLoader.load_unet(\"consolidated_s6700.safetensors\", \"fp8_e4m3fn\")[0]\n",
    "    clip = TripleCLIPLoader.load_clip(\"google_t5-v1_1-xxl_encoderonly-fp8_e4m3fn.safetensors\", \"Long-ViT-L-14-BEST-GmP-smooth-ft.safetensors\", \"clip_g.safetensors\")[0]\n",
    "    unet1, clip1 = LoraLoader.load_lora(unet, clip, \"flux/FLUX.1-Turbo-Alpha.safetensors\", 1.00, 1.00)\n",
    "    unet2, clip2 = LoraLoader.load_lora(unet1, clip1, \"flux/openflux1-v0.1.0-fast-lora.safetensors\", 0.33, 0.33)\n",
    "    unet3, clip3 = LoraLoader.load_lora(unet2, clip2, \"flux/xlabs_flux_realism_lora_comfui.safetensors\", 0.70, 0.70)\n",
    "    unet4, clip4 = LoraLoader.load_lora(unet3, clip3, \"flux/detailed_v2_flux_ntc.safetensors\", 0.70, 0.70)\n",
    "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
    "    joy_two_pipeline = Joy_caption_two_load.generate(\"Llama-3.1-8B-Lexi-Uncensored-V2\")[0]\n",
    "\n",
    "def download_file(url, save_dir, file_name):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    file_suffix = os.path.splitext(urlsplit(url).path)[1]\n",
    "    file_name_with_suffix = file_name + file_suffix\n",
    "    file_path = os.path.join(save_dir, file_name_with_suffix)\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(file_path, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    return file_path\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate(input):\n",
    "    values = input[\"input\"]\n",
    "\n",
    "    enable_image_caption = values['enable_image_caption']\n",
    "    if enable_image_caption:\n",
    "        input_image = values['input_image']\n",
    "        input_image = download_file(url=input_image, save_dir='/content/ComfyUI/input', file_name='input_image')\n",
    "        input_image = LoadImage.load_image(input_image)[0]\n",
    "        caption_type = values['caption_type']\n",
    "        caption_length = values['caption_length']\n",
    "        positive_prompt = Joy_caption_two.generate(joy_two_pipeline, input_image, caption_type, caption_length, low_vram=False)[0]\n",
    "        print(positive_prompt)\n",
    "    else:\n",
    "        positive_prompt = values['positive_prompt']\n",
    "    negative_prompt = values['negative_prompt']\n",
    "    enable_custom_lora = values['enable_custom_lora']\n",
    "    if enable_custom_lora:\n",
    "        lora_url = values['lora_url']\n",
    "        lora_file = download_file(url=lora_url, save_dir='/content/ComfyUI/models/loras', file_name='lora_file')\n",
    "        lora_file = os.path.basename(lora_file)\n",
    "        lora_strength_model = values['lora_strength_model']\n",
    "        lora_strength_clip = values['lora_strength_clip']\n",
    "        unet5, clip5 = LoraLoader.load_lora(unet4, clip4, lora_file, lora_strength_model, lora_strength_clip)\n",
    "        if os.path.exists('/content/ComfyUI/models/loras/lora_file.safetensors'):\n",
    "            os.remove('/content/ComfyUI/models/loras/lora_file.safetensors')\n",
    "    else:\n",
    "        unet5 = unet4\n",
    "        clip5 = clip4\n",
    "    seed = values['seed']\n",
    "    steps = values['steps']\n",
    "    cfg = values['cfg']\n",
    "    sampler_name = values['sampler_name']\n",
    "    scheduler = values['scheduler']\n",
    "    max_shift = values['max_shift']\n",
    "    base_shift = values['base_shift']\n",
    "    width = values['width']\n",
    "    height = values['height']\n",
    "\n",
    "    if seed == 0:\n",
    "        random.seed(int(time.time()))\n",
    "        seed = random.randint(0, 18446744073709551615)\n",
    "    print(seed)\n",
    "\n",
    "    positive = CLIPTextEncode.encode(clip5, positive_prompt)[0]\n",
    "    negative = CLIPTextEncode.encode(clip5, negative_prompt)[0]\n",
    "    unet_flux = ModelSamplingFlux.patch(unet5, max_shift, base_shift, width, height)[0]\n",
    "    noise = RandomNoise.get_noise(seed)[0]\n",
    "    guider = AdaptiveGuidance.get_guider(unet_flux, positive, negative, 1.0, cfg, uncond_zero_scale=0.0, cfg_start_pct=0.0)[0]\n",
    "    sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
    "    sigmas = BasicScheduler.get_sigmas(unet_flux, scheduler, steps, 1.0)[0]\n",
    "    latent_image = EmptyLatentImage.generate(width, height)[0]\n",
    "    sampler = DetailDaemonSamplerNode.go(sampler=sampler, detail_amount=0.2, start=0.2, end=0.9, bias=1.0, exponent=1.0, start_offset=0.0, end_offset=0.0, fade=0.0, smooth=False, cfg_scale_override=0.0)[0]\n",
    "    samples, _ = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
    "    decoded = VAEDecode.decode(vae, samples)[0].detach()\n",
    "    Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0]).save(f\"/content/flux.1-dev-distill-{seed}-tost.png\")\n",
    "\n",
    "    result = f\"/content/flux.1-dev-distill-{seed}-tost.png\"\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = { \n",
    "        \"input\": {\n",
    "            \"enable_image_caption\": True,\n",
    "            \"input_image\": \"https://files.catbox.moe/1ba4mh.png\",\n",
    "            \"caption_type\": \"Descriptive\",\n",
    "            \"caption_length\": \"long\",\n",
    "            \"positive_prompt\": \"A close-up photograph of a clay person camper riding a elephant in the forest at golden hour\",\n",
    "            \"negative_prompt\": \"sharp\",\n",
    "            \"enable_custom_lora\": True,\n",
    "            \"lora_url\": \"https://huggingface.co/TostAI/flux-1-dev-lora/resolve/main/8iiu0y.safetensors\",\n",
    "            \"lora_strength_model\": 1,\n",
    "            \"lora_strength_clip\": 1,\n",
    "            \"seed\": 0,\n",
    "            \"steps\": 8,\n",
    "            \"cfg\": 3.5,\n",
    "            \"sampler_name\": \"euler\",\n",
    "            \"scheduler\": \"beta\",\n",
    "            \"max_shift\": 1.15,\n",
    "            \"base_shift\": 0.50,\n",
    "            \"width\": 1024,\n",
    "            \"height\": 1024\n",
    "        }\n",
    "}\n",
    "image = generate(input)\n",
    "Image.open(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComfyUI-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
